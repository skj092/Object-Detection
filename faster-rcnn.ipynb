{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/skj092/Object-Detection/blob/main/pytorch_faster_rcnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"code","source":"# ref - https://www.kaggle.com/yerramvarun/fine-tuning-faster-rcnn-using-pytorch","metadata":{"id":"ay4WICCIe5xr","execution":{"iopub.status.busy":"2022-01-20T06:08:03.718743Z","iopub.execute_input":"2022-01-20T06:08:03.719312Z","iopub.status.idle":"2022-01-20T06:08:03.741739Z","shell.execute_reply.started":"2022-01-20T06:08:03.719212Z","shell.execute_reply":"2022-01-20T06:08:03.740830Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Download TorchVision repo to use some files from\n# references/detection\n!pip install pycocotools --quiet\n!git clone https://github.com/pytorch/vision.git\n!git checkout v0.3.0\n\n!cp vision/references/detection/utils.py ./\n!cp vision/references/detection/transforms.py ./\n!cp vision/references/detection/coco_eval.py ./\n!cp vision/references/detection/engine.py ./\n!cp vision/references/detection/coco_utils.py ./","metadata":{"id":"v6A4eV92e5x2","outputId":"e18c439b-4b0f-4bd0-ce6b-f7e312f6ed1d","execution":{"iopub.status.busy":"2022-01-20T06:08:03.744638Z","iopub.execute_input":"2022-01-20T06:08:03.745117Z","iopub.status.idle":"2022-01-20T06:08:57.927555Z","shell.execute_reply.started":"2022-01-20T06:08:03.745081Z","shell.execute_reply":"2022-01-20T06:08:57.926657Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Basic python and ML Libraries\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\n# for ignoring warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# We will be reading images using OpenCV\nimport cv2\n\n# xml library for parsing xml files\nfrom xml.etree import ElementTree as et\n\n# matplotlib for visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# torchvision libraries\nimport torch\nimport torchvision\nfrom torchvision import transforms as torchtrans  \nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# these are the helper libraries imported.\nfrom engine import train_one_epoch, evaluate\nimport utils\nimport transforms as T\n\n# for image augmentations\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2","metadata":{"id":"65nidIx4e5x6","execution":{"iopub.status.busy":"2022-01-20T06:08:57.929726Z","iopub.execute_input":"2022-01-20T06:08:57.929995Z","iopub.status.idle":"2022-01-20T06:09:01.941598Z","shell.execute_reply.started":"2022-01-20T06:08:57.929957Z","shell.execute_reply":"2022-01-20T06:09:01.940840Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# data downlaod from : https://www.kaggle.com/mbkinaci/fruit-images-for-object-detection/download\n# !unzip -q archive.zip ","metadata":{"id":"MH-c6ao0gZTV","execution":{"iopub.status.busy":"2022-01-20T06:09:01.943018Z","iopub.execute_input":"2022-01-20T06:09:01.943283Z","iopub.status.idle":"2022-01-20T06:09:01.947218Z","shell.execute_reply.started":"2022-01-20T06:09:01.943247Z","shell.execute_reply":"2022-01-20T06:09:01.945953Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# defining the files directory and testing directory\nfiles_dir = '../input/fruit-images-for-object-detection/train_zip/train'\ntest_dir = '../input/fruit-images-for-object-detection/test_zip/test'\n\n\nclass FruitImagesDataset(torch.utils.data.Dataset):\n\n    def __init__(self, files_dir, width, height, transforms=None):\n        self.transforms = transforms\n        self.files_dir = files_dir\n        self.height = height\n        self.width = width\n        \n        # sorting the images for consistency\n        # To get images, the extension of the filename is checked to be jpg\n        self.imgs = [image for image in sorted(os.listdir(files_dir))\n                        if image[-4:]=='.jpg']\n        \n        \n        # classes: 0 index is reserved for background\n        self.classes = ['_', 'apple','banana','orange']\n\n    def __getitem__(self, idx):\n\n        img_name = self.imgs[idx]\n        image_path = os.path.join(self.files_dir, img_name)\n\n        # reading the images and converting them to correct size and color    \n        img = cv2.imread(image_path)\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n        # diving by 255\n        img_res /= 255.0\n        \n        # annotation file\n        annot_filename = img_name[:-4] + '.xml'\n        annot_file_path = os.path.join(self.files_dir, annot_filename)\n        \n        boxes = []\n        labels = []\n        tree = et.parse(annot_file_path)\n        root = tree.getroot()\n        \n        # cv2 image gives size as height x width\n        wt = img.shape[1]\n        ht = img.shape[0]\n        \n        # box coordinates for xml files are extracted and corrected for image size given\n        for member in root.findall('object'):\n            labels.append(self.classes.index(member.find('name').text))\n            \n            # bounding box\n            xmin = int(member.find('bndbox').find('xmin').text)\n            xmax = int(member.find('bndbox').find('xmax').text)\n            \n            ymin = int(member.find('bndbox').find('ymin').text)\n            ymax = int(member.find('bndbox').find('ymax').text)\n            \n            \n            xmin_corr = (xmin/wt)*self.width\n            xmax_corr = (xmax/wt)*self.width\n            ymin_corr = (ymin/ht)*self.height\n            ymax_corr = (ymax/ht)*self.height\n            \n            boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n        \n        # convert boxes into a torch.Tensor\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        \n        # getting the areas of the boxes\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n        \n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n        # image_id\n        image_id = torch.tensor([idx])\n        target[\"image_id\"] = image_id\n\n\n        if self.transforms:\n            \n            sample = self.transforms(image = img_res,\n                                     bboxes = target['boxes'],\n                                     labels = labels)\n            \n            img_res = sample['image']\n            target['boxes'] = torch.Tensor(sample['bboxes'])\n            \n            \n            \n        return img_res, target\n\n    def __len__(self):\n        return len(self.imgs)\n\n\n# check dataset\ndataset = FruitImagesDataset(files_dir, 224, 224)\nprint('length of dataset = ', len(dataset), '\\n')\n\n# getting the image and target for a test index.  Feel free to change the index.\nimg, target = dataset[78]\nprint(img.shape, '\\n',target)","metadata":{"id":"xUq60sPwe5x8","outputId":"fb4ec888-0d3f-4ec3-8281-00f084a521e8","execution":{"iopub.status.busy":"2022-01-20T06:09:01.951102Z","iopub.execute_input":"2022-01-20T06:09:01.951536Z","iopub.status.idle":"2022-01-20T06:09:02.272166Z","shell.execute_reply.started":"2022-01-20T06:09:01.951482Z","shell.execute_reply":"2022-01-20T06:09:02.271440Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Function to visualize bounding boxes in the image\n\ndef plot_img_bbox(img, target):\n    # plot the image and bboxes\n    # Bounding boxes are defined as follows: x-min y-min width height\n    fig, a = plt.subplots(1,1)\n    fig.set_size_inches(5,5)\n    a.imshow(img)\n    for box in (target['boxes']):\n        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n        rect = patches.Rectangle((x, y),\n                                 width, height,\n                                 linewidth = 2,\n                                 edgecolor = 'r',\n                                 facecolor = 'none')\n\n        # Draw the bounding box on top of the image\n        a.add_patch(rect)\n    plt.show()\n    \n# plotting the image with bboxes. Feel free to change the index\nimg, target = dataset[2]\nplot_img_bbox(img, target)","metadata":{"id":"OvuqjsEBe5yC","outputId":"0a9c0752-c519-4921-e5cf-f4bbecd68c4a","execution":{"iopub.status.busy":"2022-01-20T06:09:02.273623Z","iopub.execute_input":"2022-01-20T06:09:02.274122Z","iopub.status.idle":"2022-01-20T06:09:02.603226Z","shell.execute_reply.started":"2022-01-20T06:09:02.274081Z","shell.execute_reply":"2022-01-20T06:09:02.601138Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\ndef get_object_detection_model(num_classes):\n\n    # load a model pre-trained pre-trained on COCO\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    \n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n\n    return model","metadata":{"id":"8X31PP5ge5yF","execution":{"iopub.status.busy":"2022-01-20T06:09:02.605633Z","iopub.execute_input":"2022-01-20T06:09:02.606564Z","iopub.status.idle":"2022-01-20T06:09:02.624294Z","shell.execute_reply.started":"2022-01-20T06:09:02.606480Z","shell.execute_reply":"2022-01-20T06:09:02.623303Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Send train=True fro training transforms and False for val/test transforms\ndef get_transform(train):\n    \n    if train:\n        return A.Compose([\n                            A.HorizontalFlip(),\n                     # ToTensorV2 converts image to pytorch tensor without div by 255\n                            ToTensorV2() \n                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n    else:\n        return A.Compose([\n                            ToTensorV2()\n                        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","metadata":{"id":"7L8f17XUe5yI","execution":{"iopub.status.busy":"2022-01-20T06:09:02.626361Z","iopub.execute_input":"2022-01-20T06:09:02.627050Z","iopub.status.idle":"2022-01-20T06:09:02.640485Z","shell.execute_reply.started":"2022-01-20T06:09:02.626977Z","shell.execute_reply":"2022-01-20T06:09:02.637718Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# use our dataset and defined transformations\ndataset = FruitImagesDataset(files_dir, 480, 480, transforms= get_transform(train=True))\ndataset_test = FruitImagesDataset(files_dir, 480, 480, transforms= get_transform(train=False))\n\n# split the dataset in train and test set\ntorch.manual_seed(1)\nindices = torch.randperm(len(dataset)).tolist()\n\n# train test split\ntest_split = 0.2\ntsize = int(len(dataset)*test_split)\ndataset = torch.utils.data.Subset(dataset, indices[:-tsize])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-tsize:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=10, shuffle=True, num_workers=4,\n    collate_fn=utils.collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=10, shuffle=False, num_workers=4,\n    collate_fn=utils.collate_fn)","metadata":{"id":"27g9fveGe5yL","execution":{"iopub.status.busy":"2022-01-20T06:09:02.643595Z","iopub.execute_input":"2022-01-20T06:09:02.644659Z","iopub.status.idle":"2022-01-20T06:09:02.678581Z","shell.execute_reply.started":"2022-01-20T06:09:02.644616Z","shell.execute_reply":"2022-01-20T06:09:02.677665Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# to train on gpu if selected.\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n\nnum_classes = 4\n\n# get the model using our helper function\nmodel = get_object_detection_model(num_classes)\n\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)","metadata":{"id":"WsQbBgtze5yP","outputId":"db1648e6-33a8-4458-8198-1071da930e3e","execution":{"iopub.status.busy":"2022-01-20T06:09:02.683169Z","iopub.execute_input":"2022-01-20T06:09:02.685579Z","iopub.status.idle":"2022-01-20T06:09:07.635848Z","shell.execute_reply.started":"2022-01-20T06:09:02.685526Z","shell.execute_reply":"2022-01-20T06:09:07.635068Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import math\nimport sys\nimport time\n\nimport torch\nimport torchvision.models.detection.mask_rcnn\nimport utils\nfrom coco_eval import CocoEvaluator\nfrom coco_utils import get_coco_api_from_dataset\n\n\ndef train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n    model.train()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n    header = f\"Epoch: [{epoch}]\"\n\n    lr_scheduler = None\n    if epoch == 0:\n        warmup_factor = 1.0 / 1000\n        warmup_iters = min(1000, len(data_loader) - 1)\n\n#         lr_scheduler = torch.optim.lr_scheduler.linear_lr(\n#             optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n#         )\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(\n            optimizer, step_size=10\n        )\n\n    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        with torch.cuda.amp.autocast(enabled=scaler is not None):\n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n\n        # reduce losses over all GPUs for logging purposes\n        loss_dict_reduced = utils.reduce_dict(loss_dict)\n        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n\n        loss_value = losses_reduced.item()\n\n        if not math.isfinite(loss_value):\n            print(f\"Loss is {loss_value}, stopping training\")\n            print(loss_dict_reduced)\n            sys.exit(1)\n\n        optimizer.zero_grad()\n        if scaler is not None:\n            scaler.scale(losses).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            losses.backward()\n            optimizer.step()\n\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n\n        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n\n    return ","metadata":{"id":"_QCkWanLe5yT","execution":{"iopub.status.busy":"2022-01-20T06:09:07.638857Z","iopub.execute_input":"2022-01-20T06:09:07.639146Z","iopub.status.idle":"2022-01-20T06:09:07.653765Z","shell.execute_reply.started":"2022-01-20T06:09:07.639100Z","shell.execute_reply":"2022-01-20T06:09:07.652910Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# training for 10 epochs\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    # training for one epoch\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, data_loader_test, device=device)","metadata":{"id":"PDtEwqHAe5yV","outputId":"77c51607-ccfd-438d-e235-62ca69aebb9a","execution":{"iopub.status.busy":"2022-01-20T06:09:07.655444Z","iopub.execute_input":"2022-01-20T06:09:07.655806Z","iopub.status.idle":"2022-01-20T06:14:20.603937Z","shell.execute_reply.started":"2022-01-20T06:09:07.655717Z","shell.execute_reply":"2022-01-20T06:14:20.603052Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# the function takes the original prediction and the iou threshold.\n\ndef apply_nms(orig_prediction, iou_thresh=0.3):\n    \n    # torchvision returns the indices of the bboxes to keep\n    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n    \n    final_prediction = orig_prediction\n    final_prediction['boxes'] = final_prediction['boxes'][keep]\n    final_prediction['scores'] = final_prediction['scores'][keep]\n    final_prediction['labels'] = final_prediction['labels'][keep]\n    \n    return final_prediction\n\n# function to convert a torchtensor back to PIL image\ndef torch_to_pil(img):\n    return torchtrans.ToPILImage()(img).convert('RGB')","metadata":{"id":"VdIUJ5JEe5yX","execution":{"iopub.status.busy":"2022-01-20T06:14:20.605844Z","iopub.execute_input":"2022-01-20T06:14:20.606164Z","iopub.status.idle":"2022-01-20T06:14:20.612324Z","shell.execute_reply.started":"2022-01-20T06:14:20.606080Z","shell.execute_reply":"2022-01-20T06:14:20.611518Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# pick one image from the test set\nimg, target = dataset_test[5]\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])[0]\n    \nprint('predicted #boxes: ', len(prediction['labels']))\nprint('real #boxes: ', len(target['labels']))","metadata":{"id":"WtEwQPjVo5KJ","outputId":"5d522db9-be2b-4eeb-8c25-a4079960fc74","execution":{"iopub.status.busy":"2022-01-20T06:14:20.613862Z","iopub.execute_input":"2022-01-20T06:14:20.614270Z","iopub.status.idle":"2022-01-20T06:14:20.694903Z","shell.execute_reply.started":"2022-01-20T06:14:20.614232Z","shell.execute_reply":"2022-01-20T06:14:20.694180Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print('EXPECTED OUTPUT')\nplot_img_bbox(torch_to_pil(img), target)","metadata":{"id":"wNoRGkwpo8hq","outputId":"93e5f310-5224-4406-93bc-d414904b8d2a","execution":{"iopub.status.busy":"2022-01-20T06:14:20.696096Z","iopub.execute_input":"2022-01-20T06:14:20.696334Z","iopub.status.idle":"2022-01-20T06:14:20.913866Z","shell.execute_reply.started":"2022-01-20T06:14:20.696301Z","shell.execute_reply":"2022-01-20T06:14:20.912389Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"print('MODEL OUTPUT')\nplot_img_bbox(torch_to_pil(img), prediction)","metadata":{"id":"CWF_Kdobo_PD","outputId":"0df4cb26-cf56-4f7c-a59f-ad9ac04ad942","execution":{"iopub.status.busy":"2022-01-20T06:14:20.915023Z","iopub.execute_input":"2022-01-20T06:14:20.915719Z","iopub.status.idle":"2022-01-20T06:14:21.326157Z","shell.execute_reply.started":"2022-01-20T06:14:20.915682Z","shell.execute_reply":"2022-01-20T06:14:21.325373Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"nms_prediction = apply_nms(prediction, iou_thresh=0.2)\nprint('NMS APPLIED MODEL OUTPUT')\nplot_img_bbox(torch_to_pil(img), nms_prediction)","metadata":{"id":"utRJTYV0pCEB","outputId":"8f566623-133c-4f25-99f2-30d3bc4b963c","execution":{"iopub.status.busy":"2022-01-20T06:14:21.327410Z","iopub.execute_input":"2022-01-20T06:14:21.327751Z","iopub.status.idle":"2022-01-20T06:14:21.566973Z","shell.execute_reply.started":"2022-01-20T06:14:21.327712Z","shell.execute_reply":"2022-01-20T06:14:21.565615Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"test_dataset = FruitImagesDataset(test_dir, 480, 480, transforms= get_transform(train=True))\n# pick one image from the test set\nimg, target = test_dataset[10]\nprint(img.shape)\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])[0]\n    \nprint('EXPECTED OUTPUT\\n')\nplot_img_bbox(torch_to_pil(img), target)\nprint('MODEL OUTPUT\\n')\nnms_prediction = apply_nms(prediction, iou_thresh=0.01)\n\nplot_img_bbox(torch_to_pil(img), nms_prediction)","metadata":{"id":"1XgTjAFlpFY5","outputId":"4f5cc278-b885-4101-dae7-4186190111df","execution":{"iopub.status.busy":"2022-01-20T06:14:21.568191Z","iopub.execute_input":"2022-01-20T06:14:21.568452Z","iopub.status.idle":"2022-01-20T06:14:22.373932Z","shell.execute_reply.started":"2022-01-20T06:14:21.568414Z","shell.execute_reply":"2022-01-20T06:14:22.373330Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# deployment ","metadata":{"id":"oGdGmZJXpIFS"}},{"cell_type":"code","source":"torch.save(model.state_dict(),'model')\nmodel.load_state_dict(torch.load('model'))\n# model.eval()","metadata":{"execution":{"iopub.status.busy":"2022-01-20T06:14:23.071910Z","iopub.execute_input":"2022-01-20T06:14:23.072200Z","iopub.status.idle":"2022-01-20T06:14:23.373086Z","shell.execute_reply.started":"2022-01-20T06:14:23.072162Z","shell.execute_reply":"2022-01-20T06:14:23.372341Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from glob import glob\nimages = glob('../input/fruit-images-for-object-detection/test_zip/test/*.jpg')\nimages[:5]","metadata":{"execution":{"iopub.status.busy":"2022-01-20T06:39:39.533705Z","iopub.execute_input":"2022-01-20T06:39:39.534454Z","iopub.status.idle":"2022-01-20T06:39:39.544834Z","shell.execute_reply.started":"2022-01-20T06:39:39.534401Z","shell.execute_reply":"2022-01-20T06:39:39.544052Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"img = Image.open(images[2])\nimg = img.resize((480, 480))\ntfms = torchtrans.ToTensor()\nimg = tfms(img)\nimg.dtype, img.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-20T06:49:49.777339Z","iopub.execute_input":"2022-01-20T06:49:49.777870Z","iopub.status.idle":"2022-01-20T06:49:49.806551Z","shell.execute_reply.started":"2022-01-20T06:49:49.777831Z","shell.execute_reply":"2022-01-20T06:49:49.805825Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"prediction = model([img.to(device)])[0]\nnms_prediction = apply_nms(prediction, iou_thresh=0.01)\nplot_img_bbox(torch_to_pil(img), nms_prediction)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T06:49:54.819154Z","iopub.execute_input":"2022-01-20T06:49:54.820002Z","iopub.status.idle":"2022-01-20T06:49:55.117306Z","shell.execute_reply.started":"2022-01-20T06:49:54.819932Z","shell.execute_reply":"2022-01-20T06:49:55.116639Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}